{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a0e10fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HomePC\\anaconda3\\envs\\new-learn-env\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: The social context in which a person lives is a big influence in self-esteem. The feeling of being worthless is caused by your inside being not matching your outside result. Therapy could be a great way to change this feeling of worthlessness.\n",
      "Answer: worthless\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "import warnings\n",
    "from transformers import logging\n",
    "\n",
    "# Suppress warnings\n",
    "logging.set_verbosity_error()\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# Helper class to simulate the expected structure by text splitter\n",
    "class Document:\n",
    "    def __init__(self, text):\n",
    "        self.page_content = text  # Text of the document\n",
    "        self.metadata = {}  # Metadata can be extended as needed\n",
    "\n",
    "# Function to clean and verify that the input is a string and replace newlines\n",
    "def prepare_text(text):\n",
    "    if isinstance(text, str) and text.strip():\n",
    "        return text.replace(\"\\n\", \" \")\n",
    "    return \"\"  # Return an empty string if the input is not valid to prevent errors\n",
    "\n",
    "# Load and prepare documents using pdfplumber\n",
    "pdf_files = [\n",
    "    \"Msiri_one.pdf\",\n",
    "    \"Msiri_two_many.pdf\"\n",
    "]\n",
    "\n",
    "documents = []\n",
    "for pdf_file in pdf_files:\n",
    "    with pdfplumber.open(pdf_file) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = prepare_text(page.extract_text())\n",
    "            if text:\n",
    "                documents.append(Document(text))  # Create Document objects\n",
    "\n",
    "if not documents:\n",
    "    raise ValueError(\"No documents loaded, check PDF paths and contents.\")\n",
    "\n",
    "# Splitting texts into manageable chunks using the custom Document class\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# Setting up the embedding model using SentenceTransformer directly\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "splits_content = [split.page_content for split in splits]\n",
    "embeddings = embedding_model.encode(splits_content)\n",
    "\n",
    "# Initialize FAISS index and add embeddings\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(np.array(embeddings))\n",
    "\n",
    "# Function to query documents based on textual query\n",
    "def query_documents(query, top_k=5):\n",
    "    query_embedding = embedding_model.encode([prepare_text(query)])\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    return [splits[i] for i in indices[0]]\n",
    "\n",
    "# Define the summarization and question answering chains using Hugging Face Pipelines\n",
    "summarization_pipeline = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", tokenizer=\"facebook/bart-large-cnn\")\n",
    "question_answering_pipeline = pipeline(\"question-answering\", model=\"bert-large-uncased-whole-word-masking-finetuned-squad\", tokenizer=\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "class CombineDocsChain:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "    def __call__(self, documents):\n",
    "        combined_text = \" \".join([doc.page_content for doc in documents])\n",
    "        return self.llm(combined_text, max_length=512, min_length=30, do_sample=False)[0]['summary_text']\n",
    "\n",
    "class QuestionGeneratorChain:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "    def __call__(self, context, question):\n",
    "        return self.llm(question=question, context=context)['answer']\n",
    "\n",
    "# Testing the document processing components\n",
    "query = \"I feel worthless\"\n",
    "relevant_docs = query_documents(query)\n",
    "combine_docs_chain = CombineDocsChain(summarization_pipeline)\n",
    "summary = combine_docs_chain(relevant_docs)\n",
    "print(f\"Summary: {summary}\")\n",
    "\n",
    "question = \"I feel stressed?\"\n",
    "question_gen_chain = QuestionGeneratorChain(question_answering_pipeline)\n",
    "answer = question_gen_chain(context=summary, question=question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8972f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (new-learn-env)",
   "language": "python",
   "name": "new-learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
